<!DOCTYPE html>
<html lang="hr-HR">

<head>
    <meta name="description" content="technology,ai">
    <meta name="author" content="Mladen Smrekar">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="robots" content="index,follow">
    <meta charset="utf-8">
    <link rel="icon" type="image/png" href="5c85b51b-6f56-4613-a39f-f0b2a272f380.png">
    <link rel="canonical" href="https://www.bug.hr">
    <title>
        Modeli umjetne inteligencije pali na testu transparentnosti
    </title>
</head>

<body>
    <h1>Modeli umjetne inteligencije pali na testu transparentnosti</h1>
    <!-- uvodni odlomak -->
    <p>
        <i>Stanfordov indeks transparentnosti ocijenio je Metu, OpenAI i druge najveće svjetske kompanije koje se bave
        umjetnom inteligencijom prema 100 pokazatelja </i>
    </p>
    <!-- autor -->
    <p>
        <b>Autor:</b> Mladen Smrekar
    </p>

    <br />
    <h2>
        Nijedan glavni programer temeljnih AI modela nije blizu obećane transparentnosti 
    </h2>
    <p>
        U srpnju i rujnu ove godine 15 najvećih <abbr title="Artificial Intelligence"><dfn>AI</dfn></abbr> kompanija potpisalo je dobrovoljne obveze za upravljanje rizicima koje
        predstavlja umjetna inteligencija. Jedna od njih odnosila se na transparentnost uz obećanje da će dijeliti
        informacije "s industrijom i s vladama, civilnim društvom i akademskom zajednicom", te da će javno izvještavati
        ​​o mogućnostima i ograničenjima svojih AI sustava. Zvuči sjajno, no što je zapravo transparentnost kad se
        govori o moćnim i prilagodljivim modelima kao što su OpenAI-jev GPT-4 ili Googleov PaLM 2?
    </p>
    <h2>
        Sve manje transparentnosti
    </h2>
    <p>
        Odgovor na to pitanje daje nam netom objavljeni izvještaj Stanfordovog centra za istraživanje temeljnih modela
        (CRFM). Deset najvećih takvih modela ocijenjeno je prema 100 različitih pokazatelja, a rezultati su blago rečeno
        nezadovoljavajući. Najveću ukupnu ocjenu na testu dobio je Metin Llama 2. No razloga za slanje nema: Llamina 54
        boda od 100 mogućih u školi bi se smatralo lošom, jedva prolaznom ocjenom. 
    </p>
    <h2>
        Faktori transparentnosti
    </h2>
    <p>
        Stotinu metrika transparentnosti uključuje upstream faktore koji se odnose na obuku, zatim informacije o
        svojstvima i funkciji modela te downstream faktore vezane uz distribuciju i upotrebu modela. 
    </p>
    <p>
        <q>Nije dovoljno da organizacija bude transparentna kada objavljuje model; stvari bi trebale biti transparentne i
        kad je riječ o resursima koji ulaze u taj model, o procjenama mogućnosti tog modela i o tome što se događa nakon
        izdavanja"</q>, smatraju istraživači sa Stanforda koji su modele ocijenili prema 100 pokazatelja. Pročešljali su sve
        javno dostupne podatke i dali modelima 1 ili 0 za svaki pokazatelj prema unaprijed određenim kriterijima.
    </p>
    <h2>
        Podaci o obuci
    </h2>
    <p>
        Podrijetlo podataka o obuci za temeljne modele postalo je vruća tema, s nekoliko tužbi u kojima se navodi da su
        AI tvrtke nezakonito uključile autorski materijal zaštićen autorskim pravima u svoje skupove podataka za obuku,
        podsjeća IEEE Spectrum. Indeks transparentnosti pokazao je da većina tvrtki nije bila otvorena u vezi sa svojim
        podacima.
    </p>
    <p>
        Model Bloomz tvrtke Hugging Face dobio je najveću ocjenu u ovoj kategoriji, 60 posto; niti jedan drugi modela
        nije postigao rezultat iznad 40 posto, a nekoliko ih je dobilo čistu nulu.
    </p>
    <h2>
        Prešućene informacije
    </h2>
    <p>
        Kompanije su također uglavnom šutjele o temi rada. Na primjer, OpenAI koristi učenje s potkrepljenjem iz ljudskih
        povratnih informacija kako bi modele poput GPT-4 naučio koji su odgovori najprikladniji i najprihvatljiviji za
        ljude. Ali većina programera ne objavljuje informacije o tome tko su ti ljudski radnici i koliko su plaćeni, a
        sumnja se i da se taj posao povjerava radnicima s niskim plaćama u zemljama poput Kenije. 
    </p>
    <p>
        Tri otvorena modela - Llama 2, Bloomz i Stable Diffusion - trenutno prednjače u transparentnosti, postižući više
        ili jednake ocjene najboljem zatvorenom modelu. No, postoji mnogo kontroverzi oko toga trebaju li uopće tako
        moćni modeli biti otvorenog koda i stoga potencijalno dostupni baš svakome.
    </p>
    <h2>
        Godišnje ažuriranje
    </h2>
    <p>
        Važno je upamtiti da čak i ako je model dobio visoku ocjenu transparentnosti u trenutnom indeksu, to ne bi nužno
        značilo da je uzor vrline umjetne inteligencije. Ako bi tvrtka otkrila da je model treniran na materijalu
        zaštićenom autorskim pravima i da su ga usavršavali radnici s plaćom nižom od minimalne, svejedno bi zaradila
        bodove za transparentnost podataka i rada.
    </p>
    <p>
        Stanfordovi istraživači svoj indeks namjeravaju ažurirati barem jednom godišnje i nadaju se da će njihova
        zapažanja koristiti zakonodavcima prilikom pisanja zakona vezanih uz umjetnu inteligenciju.
    </p>
</body>